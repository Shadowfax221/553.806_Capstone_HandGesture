{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "from models import KeyPointClassifier\n",
    "import copy \n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "landmark_list = [0.6641262173652649, 0.7233468294143677, 2.483685932475055e-07, 0.6084092855453491, 0.6979326009750366, -0.015598497353494167, 0.5637728571891785, 0.6345027685165405, -0.021485067903995514, 0.5399306416511536, 0.575805127620697, -0.027300065383315086, 0.5146738886833191, 0.5334270596504211, -0.03330542892217636, 0.5886807441711426, 0.5339487195014954, -0.0057119629345834255, 0.5640174150466919, 0.4607226550579071, -0.020116688683629036, \n",
    "0.5498967170715332, 0.4148087501525879, -0.03349391743540764, 0.5413908362388611, 0.3743138909339905, -0.043615296483039856, 0.6222752928733826, 0.5165725946426392, -0.011697640642523766, 0.6089351177215576, 0.4260062277317047, \n",
    "-0.022712308913469315, 0.6021220088005066, 0.3694627285003662, -0.034752532839775085, 0.597951352596283, 0.32305794954299927, -0.044126953929662704, 0.6556739807128906, 0.5185085535049438, -0.021214893087744713, 0.6478569507598877, 0.4312754273414612, -0.03423449769616127, 0.6435176134109497, 0.37731853127479553, -0.04411550238728523, 0.6390867233276367, 0.331074059009552, -0.051843732595443726, 0.6896733641624451, 0.5360729694366455, -0.032618217170238495, 0.6908791661262512, 0.47086232900619507, -0.04455866292119026, 0.6920442581176758, 0.4291669428348541, -0.04890189692378044, 0.6919578313827515, 0.39215558767318726, -0.05228421092033386]\n",
    "\n",
    "model_path='models/keypoint_classifier.tflite'\n",
    "num_threads=1\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=model_path,\n",
    "                                        num_threads=num_threads)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "input_details_tensor_index = input_details[0]['index']\n",
    "interpreter.set_tensor(\n",
    "            input_details_tensor_index,\n",
    "            np.array([landmark_list], dtype=np.float32))\n",
    "interpreter.invoke()\n",
    "\n",
    "output_details_tensor_index = output_details[0]['index']\n",
    "result = interpreter.get_tensor(output_details_tensor_index)\n",
    "result_index = np.argmax(np.squeeze(result))\n",
    "\n",
    "print(result_index)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "model_path = 'models/hand_landmarker.task'\n",
    "\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "HandLandmarker = mp.tasks.vision.HandLandmarker\n",
    "HandLandmarkerOptions = mp.tasks.vision.HandLandmarkerOptions\n",
    "HandLandmarkerResult = mp.tasks.vision.HandLandmarkerResult\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Create a hand landmarker instance with the live stream mode:\n",
    "def print_result(result: HandLandmarkerResult, output_image: mp.Image, timestamp_ms: int):\n",
    "    if result.hand_landmarks:\n",
    "        print(f'hand landmarker result {timestamp_ms}: {result.hand_landmarks}')\n",
    "\n",
    "options = HandLandmarkerOptions(\n",
    "    base_options=BaseOptions(model_asset_path=model_path),\n",
    "    running_mode=VisionRunningMode.LIVE_STREAM,\n",
    "    result_callback=print_result)\n",
    "\n",
    "with HandLandmarker.create_from_options(options) as landmarker:\n",
    "    while capture.isOpened():\n",
    "        ret, frame = capture.read()\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n",
    "        frame_timestamp_ms = int(time.time() * 1000)\n",
    "        landmarker.detect_async(mp_image, frame_timestamp_ms)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        cv2.imshow('Webcam', image)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "        \n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'mediapipe.python.solutions.hands' from 'c:\\\\Users\\\\Ian\\\\git\\\\553.806_Capstone_HandGesture\\\\.venv\\\\Lib\\\\site-packages\\\\mediapipe\\\\python\\\\solutions\\\\hands.py'>\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_hands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5) as hands:\n",
    "    while capture.isOpened():\n",
    "        ret, frame = capture.read()\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        detected_image = hands.process(image)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        if detected_image.multi_hand_landmarks:\n",
    "            for hand_lms in detected_image.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(image, hand_lms,\n",
    "                                            mp_hands.HAND_CONNECTIONS,\n",
    "                                            landmark_drawing_spec=mp.solutions.drawing_utils.DrawingSpec(\n",
    "                                                color=(255, 0, 255), thickness=4, circle_radius=2),\n",
    "                                            connection_drawing_spec=mp.solutions.drawing_utils.DrawingSpec(\n",
    "                                                color=(20, 180, 90), thickness=2, circle_radius=2)\n",
    "                                            )\n",
    "    \n",
    "        cv2.imshow('Webcam', image)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
